è¿™ä¸ªä»»åŠ¡æ¶‰åŠä» 1D è¶…å£°æ³¢ä¿¡å·é¢„æµ‹ 2D å£°é€Ÿåˆ†å¸ƒå›¾åƒï¼Œå»ºè®®ä½¿ç”¨ **CNN + Transformer + U-Net** ç»“åˆçš„æ¶æ„ã€‚è¯¥æ¶æ„çš„å…³é”®éƒ¨åˆ†åŒ…æ‹¬ï¼š

1. **CNN å±‚**ï¼šç”¨äºæå–æ—¶åºä¿¡å·çš„å±€éƒ¨ç‰¹å¾ã€‚
2. **Transformer å±‚**ï¼šç”¨äºæ•æ‰ä¿¡å·çš„å…¨å±€ä¾èµ–å…³ç³»ï¼Œæå‡å»ºæ¨¡èƒ½åŠ›ã€‚
3. **U-Net ç»“æ„**ï¼šç”¨äºä»ç‰¹å¾ä¸­ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ 2D å£°é€Ÿåˆ†å¸ƒå›¾ã€‚

### ä»£ç å®ç°
ä»¥ä¸‹æ˜¯ PyTorch ç‰ˆæœ¬çš„å®ç°ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from torchvision.models.resnet import resnet18

# CNN æ¨¡å—ç”¨äºåˆæ­¥ç‰¹å¾æå–
class CNNFeatureExtractor(nn.Module):
    def __init__(self, in_channels=128, out_channels=256):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv1d(in_channels, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(128, out_channels, kernel_size=3, padding=1),
            nn.ReLU()
        )
    
    def forward(self, x):
        return self.conv(x)  # (B, 256, 1024)

# Transformer æ¨¡å—ç”¨äºå…¨å±€ç‰¹å¾å»ºæ¨¡
class TransformerEncoder(nn.Module):
    def __init__(self, seq_len=1024, embed_dim=256, num_heads=8, depth=4):
        super().__init__()
        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, embed_dim))
        self.encoder_layers = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),
            num_layers=depth
        )
    
    def forward(self, x):
        x = rearrange(x, 'b c l -> b l c')  # è½¬æ¢ä¸º (B, 1024, 256)
        x += self.pos_embedding
        x = self.encoder_layers(x)  # (B, 1024, 256)
        return rearrange(x, 'b l c -> b c l')  # è¿˜åŸå½¢çŠ¶ (B, 256, 1024)

# U-Net è§£ç å™¨ç”¨äºç”Ÿæˆ 2D å›¾åƒ
class UNetDecoder(nn.Module):
    def __init__(self, in_channels=256, out_size=(384, 384)):
        super().__init__()
        self.upsample = nn.Sequential(
            nn.ConvTranspose2d(in_channels, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),
        )
        self.out_size = out_size

    def forward(self, x):
        x = self.upsample(x)  # ä¸Šé‡‡æ ·è‡³æ¥è¿‘ç›®æ ‡å¤§å°
        return F.interpolate(x, size=self.out_size, mode='bilinear', align_corners=False)

# æ•´ä½“ç½‘ç»œ
class UltrasoundModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.cnn = CNNFeatureExtractor()
        self.transformer = TransformerEncoder()
        self.unet = UNetDecoder()

    def forward(self, x):
        x = self.cnn(x)  # (B, 256, 1024)
        x = self.transformer(x)  # (B, 256, 1024)
        x = x.unsqueeze(-1)  # (B, 256, 1024, 1)
        x = self.unet(x)  # (B, 1, 384, 384)
        return x

# æµ‹è¯•
model = UltrasoundModel()
input_tensor = torch.randn(4, 128, 1024)  # Batch size = 4
output = model(input_tensor)
print(output.shape)  # æœŸæœ›è¾“å‡º: (4, 1, 384, 384)
```

### å…³é”®ç‚¹ï¼š
1. **CNN æå–æ—¶åºç‰¹å¾**ï¼Œå°† 128 ç»´é€šé“å‹ç¼©ä¸º 256 ç»´ã€‚
2. **Transformer å…¨å±€å»ºæ¨¡**ï¼Œå¢å¼ºç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚
3. **U-Net è¿›è¡Œè§£ç **ï¼Œå°† 1D ç‰¹å¾æ˜ å°„åˆ° 2D å£°é€Ÿåˆ†å¸ƒå›¾åƒã€‚

è¿™ä¸ªæ¨¡å‹å¯ä»¥ç”¨äºè¶…å£°æˆåƒä»»åŠ¡ï¼Œæ¯”å¦‚åŒ»å­¦æˆåƒã€ææ–™æ— æŸæ£€æµ‹ç­‰ã€‚ä½ å¯ä»¥ç”¨ MSE Loss æˆ– SSIM Loss è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯è®©é¢„æµ‹çš„ 2D å£°é€Ÿå›¾åƒä¸çœŸå®æ•°æ®åŒ¹é…ã€‚


## åŸºäºåŸæ¥çš„ä»£ç æ ¼å¼å®ç°ï¼š
ä»¥ä¸‹æ˜¯åŸºäºä½ çš„æ ¼å¼å®ç°çš„ **CNN + Transformer + U-Net** ç»“åˆæ¨¡å‹ï¼Œé€‚ç”¨äºä»è¶…å£°æ³¢ä¿¡å·ï¼ˆ1Dï¼‰é¢„æµ‹å£°é€Ÿåˆ†å¸ƒå›¾åƒï¼ˆ2Dï¼‰ã€‚ä½¿ç”¨ Keras è¿›è¡Œå®ç°ã€‚

---

### **Encoderï¼ˆåŒ…å« CNN + Transformerï¼‰**
```python
import tensorflow as tf
from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, Dropout, MaxPool2D, 
                                     UpSampling2D, Concatenate, GaussianNoise, Resizing)
from tensorflow.keras.models import Model
from tensorflow.keras.initializers import GlorotUniform
import tensorflow.keras.backend as K

class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation="relu"), 
            tf.keras.layers.Dense(embed_dim)
        ])
        self.norm1 = tf.keras.layers.LayerNormalization()
        self.norm2 = tf.keras.layers.LayerNormalization()
        self.dropout1 = tf.keras.layers.Dropout(dropout)
        self.dropout2 = tf.keras.layers.Dropout(dropout)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.norm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.norm2(out1 + ffn_output)

def encoder_block(input_shape=(128, 1024, 1), dropout=0.5, transformer_depth=2):
    rf_input = Input(shape=input_shape)
    noisy_rf_input = GaussianNoise(1)(rf_input)

    # CNN éƒ¨åˆ†
    x = Conv2D(32, (3, 15), (1, 2), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(noisy_rf_input)
    x = BatchNormalization()(x, training=True)
    x = Conv2D(32, (3, 13), (1, 2), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = BatchNormalization()(x, training=True)
    x = Conv2D(32, (3, 11), (1, 2), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = BatchNormalization()(x, training=True)
    x = Conv2D(32, (3, 9), (1, 2), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = Dropout(dropout)(x)
    x = BatchNormalization()(x, training=True)

    # Transformer éƒ¨åˆ†
    x_shape = K.int_shape(x)
    x = tf.keras.layers.Reshape((x_shape[1] * x_shape[2], x_shape[3]))(x)
    for _ in range(transformer_depth):
        x = TransformerBlock(embed_dim=x_shape[3], num_heads=8, ff_dim=256)(x)
    x = tf.keras.layers.Reshape((x_shape[1], x_shape[2], x_shape[3]))(x)

    # ç¼–ç éƒ¨åˆ†ç»§ç»­
    x = Conv2D(64, (3, 7), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = MaxPool2D((2, 2), strides=(2, 2))(x)
    x = Dropout(dropout)(x)
    x = BatchNormalization()(x, training=True)

    x = Conv2D(128, (3, 5), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = MaxPool2D((2, 2), strides=(2, 2))(x)
    x = BatchNormalization()(x, training=True)

    x = Conv2D(256, (3, 3), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = MaxPool2D((2, 2), strides=(2, 2))(x)
    x = Dropout(dropout)(x)
    x = BatchNormalization()(x, training=True)

    model = Model(rf_input, x)
    model.summary()
    return model
```

---

### **Decoderï¼ˆU-Net ç»“æ„ï¼‰**
```python
def decoder_block(encoder_block):
    x = Conv2D(128, (3, 3), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(encoder_block.get_layer(index=-1).output)
    x = UpSampling2D((2, 2), interpolation='bilinear')(x)
    x = BatchNormalization()(x, training=True)
    x = Concatenate()([encoder_block.get_layer(index=-4).output, x])  

    x = Conv2D(64, (3, 5), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = UpSampling2D((2, 2), interpolation='bilinear')(x)
    x = BatchNormalization()(x, training=True)
    x = Concatenate()([encoder_block.get_layer(index=-7).output, x])  

    x = Conv2D(32, (3, 7), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = UpSampling2D((2, 2), interpolation='bilinear')(x)
    x = BatchNormalization()(x, training=True)
    x = Concatenate()([encoder_block.get_layer(index=-10).output, x])  

    x = Conv2D(32, (3, 9), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = UpSampling2D((2, 2), interpolation='bilinear')(x)
    x = BatchNormalization()(x, training=True)

    x = Conv2D(32, (3, 11), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = Resizing(384, 384)(x)

    x = Conv2D(32, (3, 3), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform())(x)
    x = BatchNormalization()(x, training=True)

    x = Conv2D(1, (1, 1), (1, 1), padding='same', activation='sigmoid', kernel_initializer=GlorotUniform())(x)

    model = Model(encoder_block.input, x)
    model.summary()
    return model
```

---

### **æ„å»ºå®Œæ•´æ¨¡å‹**
```python
# æ„å»º Encoder
encoder_model = encoder_block(input_shape=(128, 1024, 1), dropout=0.5)

# æ„å»º Decoder
full_model = decoder_block(encoder_model)

# ç¼–è¯‘æ¨¡å‹
full_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
```

---

### **æ€»ç»“**
- **CNN** å¤„ç†å±€éƒ¨æ—¶åºç‰¹å¾
- **Transformer** æ•æ‰å…¨å±€ä¿¡æ¯
- **U-Net** è¿›è¡Œå›¾åƒæ¢å¤
- **ç¬¦åˆä½ çš„ç¼–ç æ ¼å¼**

è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°å°†è¶…å£°æ³¢ä¿¡å·æ˜ å°„åˆ° 2D å£°é€Ÿåˆ†å¸ƒå›¾ã€‚ä½ å¯ä»¥ç”¨ `MSE Loss` è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡ `SSIM` è¿›è¡Œè¯„ä¼°ã€‚ğŸš€
