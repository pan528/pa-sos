![image](https://github.com/user-attachments/assets/8e628c1e-5af8-450f-935c-0ae70f672e0c)# 1. 关于cnn-transformer-unet的大联合网络：跑不动    
     
## 1.1 skip connection进行concatenate时通道数没对齐——>已解决
## 1.2 如何增加if __name__ == '__main__':测试
## 1.3 如何检查每一层的张量output shape
## 1.4 如何打印结构
# 2. unet-transformer
姐们的本质是unet。。。当然文章里没提！**可以把这个当作新的点去给小老板汇报**
## 2.1 transformer引入位置不同可以做文章，encoder&decoder可以各放一个，看看放在哪里的效果好
### 2.1.1 encoder-with-transformer
![decoder_model_with_transformer](https://github.com/user-attachments/assets/715195ef-09ea-4cae-8d21-73d1d659c5e8)

## 2.2 skip connection位置的选择——>已解决，可以正常跑


在解码器中选择跳跃连接的层时，通常需要选择编码器中不同分辨率的特征图，以便结合低层次的细节信息和高层次的语义信息。以下是基于您提供的编码器结构的分析和建议：

---

### **1. 跳跃连接的选择原则**
- **分辨率对齐**：解码器中每次上采样后的特征图分辨率应与编码器中某一层的特征图分辨率一致。
- **多层次特征融合**：选择编码器中不同分辨率的特征图（浅层、中层、深层），以结合细节信息和语义信息。
- **对称性**：跳跃连接的层次通常与解码器的上采样阶段对齐。

---

### **2. 编码器层的分辨率**
根据您提供的编码器层信息，以下是各层的分辨率和通道数：

| 层名称                  | 输出形状                  | 描述                     |
|-------------------------|--------------------------|--------------------------|
| `input_1`              | `(None, 128, 1024, 1)`  | 输入层                   |
| `gaussian_noise`       | `(None, 128, 1024, 1)`  | 添加噪声                 |
| `encoder_conv1`        | `(None, 128, 512, 32)`  | 浅层特征图（高分辨率）   |
| `batch_normalization`  | `(None, 128, 512, 32)`  | 批归一化                 |
| `encoder_conv2`        | `(None, 128, 256, 64)`  | 中层特征图               |
| `batch_normalization_1`| `(None, 128, 256, 64)`  | 批归一化                 |
| `reshape_1`            | `(None, 128, 256, 64)`  | Transformer 输出         |
| `encoder_conv3`        | `(None, 128, 128, 128)` | 深层特征图               |
| `encoder_pool1`        | `(None, 64, 64, 128)`   | 深层特征图（低分辨率）   |

---

### **3. 建议的跳跃连接层**
根据分辨率和语义信息的层次，建议选择以下层进行跳跃连接：

#### **(1) `encoder_conv1`**
- **输出形状**：`(None, 128, 512, 32)`
- **描述**：浅层特征图，分辨率较高，包含输入的细节信息（如边缘和纹理）。
- **跳跃连接作用**：帮助解码器恢复高分辨率的细节信息。

#### **(2) `encoder_conv2`**
- **输出形状**：`(None, 128, 256, 64)`
- **描述**：中层特征图，分辨率适中，包含更多的语义信息。
- **跳跃连接作用**：帮助解码器逐步恢复中间分辨率的特征。

#### **(3) `encoder_pool1`**
- **输出形状**：`(None, 64, 64, 128)`
- **描述**：深层特征图，分辨率较低，包含高层语义信息。
- **跳跃连接作用**：帮助解码器恢复语义信息，同时逐步提高分辨率。

---

### **4. 解码器中的跳跃连接实现**
在解码器中，每次上采样后，将对应的编码器特征图通过跳跃连接与解码器的特征图拼接。以下是修改后的 `decoder_block` 示例代码：

```python
def decoder_block(encoder_block):
    '''
    :param encoder_block: 编码器模型
    :return: 解码器模型
    '''
    # block 1: 对应 encoder_pool1
    x = Conv2D(128, (3, 3), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform(), name='decoder_conv1')(encoder_block.get_layer('encoder_pool1').output)
    x = UpSampling2D((2, 2), interpolation='bilinear')(x)
    x = BatchNormalization()(x, training=True)
    x = Concatenate()([encoder_block.get_layer('encoder_conv3').output, x])  # 跳跃连接 encoder_conv3

    # block 2: 对应 encoder_conv2
    x = Conv2D(64, (3, 5), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform(), name='decoder_conv2')(x)
    x = UpSampling2D((2, 2), interpolation='bilinear')(x)
    x = BatchNormalization()(x, training=True)
    x = Concatenate()([encoder_block.get_layer('encoder_conv2').output, x])  # 跳跃连接 encoder_conv2

    # block 3: 对应 encoder_conv1
    x = Conv2D(32, (3, 7), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform(), name='decoder_conv3')(x)
    x = UpSampling2D((2, 2), interpolation='bilinear')(x)
    x = BatchNormalization()(x, training=True)
    x = Concatenate()([encoder_block.get_layer('encoder_conv1').output, x])  # 跳跃连接 encoder_conv1

    # block 4: 最终恢复到目标分辨率
    x = Conv2D(32, (3, 9), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform(), name='decoder_conv4')(x)
    x = UpSampling2D((2, 2), interpolation='bilinear')(x)
    x = BatchNormalization()(x, training=True)

    # block 5: 调整到最终输出分辨率
    x = Conv2D(32, (3, 11), (1, 1), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform(), name='decoder_conv5')(x)
    x = Resizing(384, 384)(x)

    # block 6: 最终输出
    x = Conv2D(1, (1, 1), (1, 1), padding='same', activation='sigmoid', kernel_initializer=GlorotUniform(), name='decoder_output')(x)

    model = Model(encoder_block.input, x)
    return model
```

---

### **5. 总结**
在解码器中，建议选择以下编码器层进行跳跃连接：
1. **`encoder_conv1`**：浅层特征图，分辨率高，包含细节信息。
2. **`encoder_conv2`**：中层特征图，分辨率适中，包含语义信息。
3. **`encoder_pool1`**：深层特征图，分辨率低，包含高层语义信息。

通过这些跳跃连接，解码器可以结合不同层次的特征，逐步恢复高分辨率的输出，同时保留输入的细节信息和语义信息。

## 2.3 超出内存
 ![image](https://github.com/user-attachments/assets/88fcbd53-8670-4ab7-978a-a0afe440c820)
### 问题描述：Allocator (mklcpu) ran out of memory trying to allocate 320.00GiB 表示模型在运行时尝试分配一个非常大的张量（320GB），导致内存不足。
这有点太大了没必要
### 采取策略：批次改为4，num_heads=4
结果：还是有点恐怖，这甚至没开始练多少：     
     ![image](https://github.com/user-attachments/assets/5b9e1c60-a7b9-4b1f-aa6a-e873828a72ef)

可能原因：
多头注意力层：multi_head_attention 层的输入序列长度过长，导致内存需求过高。
批次大小过大：当前批次大小为 10，在高分辨率输入下可能导致内存不足。
输入数据分辨率过高：输入数据的分辨率可能过高，导致模型的中间张量过大。
从错误信息来看，问题是由于 **内存不足（OOM, Out of Memory）** 导致的。具体原因是 `multi_head_attention` 层在计算时尝试分配一个非常大的张量，形状为 `[10, 8, 32768, 32768]`，这显然超出了系统的内存限制。

---

### **1. 问题原因**
- **张量形状过大**：`multi_head_attention` 层的输入序列长度为 `32768`，这可能是由于 `Reshape` 操作将特征图展平为序列时，未正确处理序列长度。
- **批次大小过大**：当前批次大小为 `10`，在序列长度较大的情况下，批次大小会进一步放大内存需求。
- **多头注意力的头数过多**：`num_heads=8`，每个头的计算会进一步增加内存需求。

---

### **2. 解决方法**
#### **(1) 减少序列长度**
- 检查 `Reshape` 操作是否正确。例如，`Reshape` 将特征图展平为 `(batch_size, height * width, channels)`，如果 `height * width` 太大，会导致序列长度过长。
- 可以通过减少输入图像的分辨率或在 `Reshape` 之前添加降采样操作（如 `MaxPooling2D` 或 `Conv2D`）。

#### **修改代码**
在 `encoder_block_with_transformer` 中，添加降采样操作以减少序列长度：
```python
x = Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='LeakyReLU')(x)  # 降低分辨率
x_shape = K.int_shape(x)
x = tf.keras.layers.Reshape((x_shape[1] * x_shape[2], x_shape[3]))(x)  # 将特征图展平为序列
```

---

#### **(2) 减少批次大小** ✔
- 将批次大小从 `10` 减小到更小的值（如 `2` 或 `4`），以减少内存需求。

#### 修改代码
在 `train` 函数中，将 `batch_size` 参数设置为较小的值：
```python
train(X_train, y_train, input_shape=(128, 1024, 1), dropout=0.5, learning_rate=1e-4, momentum=0.9,
      decay=1e-5, batch_size=2, epochs=50, validation_split=0.2, shuffle=True, model_name=model_name, model_flag='no_pre_train')
```

---

#### **(3) 减少多头注意力的头数** ✔
- 将 `num_heads` 参数从 `8` 减小到更小的值（如 `4` 或 `2`），以减少内存需求。

#### 修改代码
在 `encoder_block_with_transformer` 中，将 `num_heads` 参数设置为较小的值：
```python
encoder_block_with_transformer(input_shape=input_shape, dropout=dropout, transformer_depth=2, num_heads=4, ff_dim=256)
```

---

#### **(4) 使用混合精度训练** ✔
- 启用混合精度训练（Mixed Precision Training），以减少内存占用。
- 混合精度训练会将部分计算切换为 `float16`，从而显著降低内存需求。

#### 修改代码 
在脚本开头启用混合精度训练：
```python
from tensorflow.keras.mixed_precision import experimental as mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)
```

---

#### **(5) 检查输入数据的形状**
- 确保输入数据的形状与模型的预期一致。如果输入数据的分辨率过高，可以尝试降低分辨率。

#### 修改代码
在加载数据时，调整输入数据的分辨率：
```python
data_input = tf.image.resize(data_input, (64, 512))  # 调整分辨率
```

---

### **3. 综合修改后的代码示例**
以下是综合上述修改后的代码：
```python
def encoder_block_with_transformer(input_shape=None, dropout=0.5, transformer_depth=2, num_heads=4, ff_dim=256):
    '''
    :param input_shape: 输入形状
    :param dropout: Dropout 比例
    :param transformer_depth: Transformer 堆叠的层数
    :param num_heads: 多头注意力的头数
    :param ff_dim: 前馈网络的隐藏层维度
    :return: 带有 Transformer 的编码器
    '''
    # 输入
    rf_input = Input(shape=input_shape)
    noisy_rf_input = GaussianNoise(1)(rf_input)

    # CNN 部分
    x = Conv2D(32, (3, 15), (1, 2), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform(), name='encoder_conv1')(noisy_rf_input)
    x = BatchNormalization()(x, training=True)
    x = Conv2D(64, (3, 13), (1, 2), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform(), name='encoder_conv2')(x)
    x = BatchNormalization()(x, training=True)

    # 降采样以减少序列长度
    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='LeakyReLU')(x)  # 降低分辨率
    x_shape = K.int_shape(x)
    x = tf.keras.layers.Reshape((x_shape[1] * x_shape[2], x_shape[3]))(x)  # 将特征图展平为序列

    # Transformer 部分
    for _ in range(transformer_depth):
        x = transformer_block(x, embed_dim=x_shape[3], num_heads=num_heads, ff_dim=ff_dim)
    x = tf.keras.layers.Reshape((x_shape[1], x_shape[2], x_shape[3]))(x)  # 恢复特征图形状

    # 编码器继续
    x = Conv2D(128, (3, 11), (1, 2), padding='same', activation='LeakyReLU', kernel_initializer=GlorotUniform(), name='encoder_conv3')(x)
    x = MaxPool2D((2, 2), strides=(2, 2), name='encoder_pool1')(x)
    x = Dropout(dropout, name='dropout3')(x)
    x = BatchNormalization()(x, training=True)

    model = Model(rf_input, x)
    model.summary()
    return model
```

---

### **4. 总结**
- **问题原因**：`multi_head_attention` 层的输入序列长度过长，导致内存不足。
- **解决方法**：
  1. 降低序列长度（通过降采样）。
  2. 减小批次大小。
  3. 减少多头注意力的头数。
  4. 启用混合精度训练。
  5. 检查输入数据的分辨率。
- **建议**：优先尝试降低序列长度和批次大小，这通常是最有效的解决方案。

## 针对 TensorFlow 的优化建议
如果你是用 **TensorFlow** 训练模型，并且 **GPU 利用率较低**，那么可能的瓶颈主要在于 **数据加载、显存管理或计算优化**。以下是针对 TensorFlow 的优化建议：  

---

### **1. 确保 TensorFlow 正确使用 GPU**
你可以运行以下代码检查 TensorFlow 是否正确检测到 GPU：
```python
import tensorflow as tf

# 查看可用 GPU
print("Num GPUs Available:", len(tf.config.experimental.list_physical_devices('GPU')))

# 确保 TensorFlow 运行在 GPU 上
print("TensorFlow is using GPU:", tf.test.is_gpu_available())
```
如果输出显示 **没有 GPU**，可能需要安装 `tensorflow-gpu` 版本：
```bash
pip install tensorflow-gpu
```
**RTX 4090 建议使用 TensorFlow 2.10+ 以及 CUDA 11.8 + cuDNN 8.6**，否则可能会遇到兼容性问题。

---

### **2. 确保 TensorFlow 正确分配显存**
**默认情况下，TensorFlow 可能不会使用所有 GPU 显存**，你可以尝试以下方式：
```python
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)  # 允许按需分配显存
        print("GPU memory growth set to True")
    except RuntimeError as e:
        print(e)
```
**或者手动设置显存使用上限**：
```python
tf.config.experimental.set_virtual_device_configuration(
    gpus[0],
    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=20480)]  # 限制显存 20GB
)
```
如果不设置，TensorFlow 可能会默认 **预分配几乎所有显存**，导致 GPU 资源浪费或 OOM。

---

### **3. 加速数据加载（提升 GPU 利用率）**
如果 **GPU 利用率低**，很可能是 **数据加载速度慢**，导致 GPU 在等待数据。可以用 `tf.data` 加速：
```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.shuffle(1000).batch(32).prefetch(AUTOTUNE)
```
`prefetch(AUTOTUNE)` 让数据加载与模型计算并行，提高 GPU 利用率。

---

### **4. 使用混合精度训练（减少显存占用，提高计算速度）**
对于 **RTX 4090** 这样的 **Ampere 架构 GPU**，启用 **混合精度** 训练可以加速计算：
```python
from tensorflow.keras import mixed_precision

mixed_precision.set_global_policy('mixed_float16')  # 启用 float16 计算
```
这样可以减少显存占用，并加速计算（部分操作加速 1.5~2 倍）。

---

### **5. 确保 TensorFlow 计算在 GPU 上**
如果你的代码仍然 **在 CPU 运行**，可以用以下方法强制使用 GPU：
```python
with tf.device('/GPU:0'):
    model.fit(train_dataset, epochs=10)
```
你也可以检查 TensorFlow 是否在使用 GPU：
```python
print("Is TensorFlow using GPU:", tf.config.list_logical_devices('GPU'))
```

---

### **6. 监控 GPU 运行状态**
你可以使用 `nvidia-smi` 监控 GPU 利用率：
```bash
nvidia-smi -l 2
```
或者在 Python 中监控 GPU 负载：
```python
import tensorflow as tf
print(tf.config.experimental.get_memory_info('GPU:0'))
```

---

### **7. 结论**
你的 GPU 显存占用很高（20.3/24GB），但 GPU 计算利用率只有 10%，可能的原因：
1. **数据加载太慢** → 解决方案：`prefetch(AUTOTUNE)`
2. **显存预分配问题** → 解决方案：`set_memory_growth(True)`
3. **计算优化不足** → 解决方案：使用 **混合精度 `mixed_float16`**
4. **任务可能跑在 CPU 上** → 解决方案：检查 `tf.device('/GPU:0')`

